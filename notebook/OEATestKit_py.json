{
	"name": "OEATestKit_py",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark3p3sm",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false
		},
		"metadata": {
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			}
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run /OEA_py"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"oea.set_workspace(workspace)"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import pyspark.sql.functions as f\r\n",
					"import pandas as pd\r\n",
					"import numpy as np\r\n",
					"\r\n",
					"class OEATestKit:\r\n",
					"    #---------------------------- Dataframe Generators ----------------------------\r\n",
					"    \"\"\"\r\n",
					"    Gets a dataframe from a raw data file\r\n",
					"\r\n",
					"    Arguments:\r\n",
					"        primary_key(string): the primary key for the entity\r\n",
					"        entity_path (string): the path to the entity\r\n",
					"        options (object): options about how to infer the schema\r\n",
					"    \"\"\"\r\n",
					"    def get_raw_dataframe(self, primary_key, entity_path, options):\r\n",
					"        primary_key = oea.fix_column_name(primary_key) # fix the column name, in case it has a space in it or some other invalid character\r\n",
					"        ingested_path = f'stage2/Ingested/{entity_path}'\r\n",
					"        raw_path = f'stage1/Transactional/{entity_path}'\r\n",
					"        batch_type, source_data_format = oea.get_batch_info(raw_path)\r\n",
					"        source_url = oea.to_url(f'{raw_path}/{batch_type}_batch_data')\r\n",
					"\r\n",
					"        if batch_type == 'snapshot'or batch_type=='additive': source_url = f'{source_url}/{oea.get_latest_folder(source_url)}'\r\n",
					"\r\n",
					"        if options == None: options = {}\r\n",
					"        options['format'] = source_data_format # eg, 'csv', 'json'\r\n",
					"        if source_data_format == 'csv' and (not 'header' in options or options['header'] == None): options['header'] = True  # default to expecting a header in csv files\r\n",
					"\r\n",
					"        spark.sql(\"set spark.sql.streaming.schemaInference=true\")\r\n",
					"        return spark.read.format('delta').load(oea.to_url(source_url), **options)\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    Gets a dataframe from a lake database\r\n",
					"\r\n",
					"    Arguments:\r\n",
					"        item (string): the name of the entity to retrieve from the lake database\r\n",
					"        stage (int): the number of the stage to get the dataframe from\r\n",
					"        type_id (string): The type identifier.  i for ingest or r for refine\r\n",
					"        collection: The name of the collection, aka the name of the parentmost folder\r\n",
					"        version (string): The version number\r\n",
					"    \"\"\"\r\n",
					"    def get_lake_dataframe(self,item, stage, type_id, collection, version, path_modifier=None):\r\n",
					"        version_delimiter = \"p\"\r\n",
					"        version_split = version.split(\".\")\r\n",
					"        namespace = f\"ldb_{oea.workspace}_s{stage}{type_id.lower()}_{collection.lower()}_v{version_delimiter.join(version_split)}\"\r\n",
					"        if path_modifier:\r\n",
					"            namespace += f'_{path_modifier}'\r\n",
					"        return spark.sql(f\"SELECT * FROM {namespace}.{item.lower()}\")\r\n",
					"\r\n",
					"    #------------------------------ Utility Methods --------------------------------\r\n",
					"    \"\"\"\r\n",
					"    This method checks if all of the rows in the first dataframe exist in the second dataframe, by\r\n",
					"    comparing the dataframes on the primary key.  Returns True if all the entities in df1 appear in df2\r\n",
					"\r\n",
					"    Arguments:\r\n",
					"        df1 (Spark.Dataframe): Dataframe of the first set of data\r\n",
					"        df2 (Spark.Dataframe): Dataframe of the second set of data\r\n",
					"        primary_key (string | string[]): the primary key(s) used for comparison \r\n",
					"    \"\"\"\r\n",
					"    def is_subset(self, df1, df2, primary_key):\r\n",
					"        merged_df = pd.merge(df1.toPandas(), df2.toPandas(), on=primary_key, how=\"left\", indicator=\"exists\")\r\n",
					"        merged_df['exists'] = np.where(merged_df.exists == 'both', True, False)\r\n",
					"        return merged_df[merged_df.exists == False].shape[0] == 0\r\n",
					"    \r\n",
					"    \"\"\"\r\n",
					"    This method checks if the df has any duplicates using the primary_key as a comparison.  Returns True if there are duplicates\r\n",
					"    else it returns false\r\n",
					"\r\n",
					"    Arguments:\r\n",
					"        df (Spark.Dataframe): the dataframe to test\r\n",
					"        primary_key (string | string[]): The primary key column(s)\r\n",
					"    \"\"\"\r\n",
					"    def has_duplicates(self, df, primary_key):\r\n",
					"        grouped_df = df.groupBy(primary_key).count()\r\n",
					"        return grouped_df.where(f.col('count') > 1).count() > 0\r\n",
					"    \r\n",
					"    #------------------------- Test Cases --------------------------------------\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    Tests to confirm that the raw dataset is a subset of the lake database, thus ensuring all the entities in the raw data\r\n",
					"    exist in the lake database\r\n",
					"    \"\"\"\r\n",
					"    def test_raw_data_is_subset_of_lake(self, collection, version, item, primary_key, options, path_modifier=None):\r\n",
					"        #Get the lake database as a dataframe\r\n",
					"        lake_df = self.get_lake_dataframe(item, 2, \"i\", collection, version, path_modifier)\r\n",
					"\r\n",
					"        #get the raw data as a dataframe\r\n",
					"        entity_path = f'{collection}/v{version}/'\r\n",
					"        entity_path += f'{path_modifier}/{item}' if path_modifier else item\r\n",
					"        raw_df = self.get_raw_dataframe(primary_key, entity_path, options)\r\n",
					"\r\n",
					"        #check if the raw data exists in the lake database\r\n",
					"        assert self.is_subset(raw_df, lake_df, primary_key)\r\n",
					"    \r\n",
					"    def test_has_duplicates(self, collection, version, item, primary_key, path_modifier=None):\r\n",
					"        #Get the lake database as a dataframe\r\n",
					"        lake_df = self.get_lake_dataframe(item, 2, \"i\", collection, version, path_modifier)\r\n",
					"        assert self.has_duplicates(lake_df, primary_key) != True\r\n",
					"        \r\n",
					"oea_test_kit = OEATestKit()\r\n"
				]
			}
		]
	}
}